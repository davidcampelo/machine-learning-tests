{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Decision Trees\n",
    "\n",
    "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.[1]\n",
    "\n",
    "This technique employs the logic in which the subsequent predictors learn from the mistakes of the previous predictors. Therefore, the observations have an unequal probability of appearing in subsequent models and ones with the highest error appear most. The predictors can be chosen from a range of models like decision trees, regressors, classifiers etc. Because new predictors are learning from mistakes committed by previous predictors, it takes less time/iterations to reach close to actual predictions. But we have to choose the stopping criteria carefully or it could lead to overfitting on training data. Gradient Boosting is an example of boosting algorithm.\n",
    "\n",
    "\n",
    "In this notebook we're using the home prices using dataset from Iowa, USA.\n",
    "\n",
    "(Inspired by a tutorial published at www.kaggle.com)\n",
    "\n",
    "[1] https://en.wikipedia.org/wiki/Gradient_boosting   \n",
    "[2] https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1d616c3cc069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "    \n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e9e2778cd11e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/iowa_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/iowa_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Drop houses where the target value for prediction is not defined (!)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SalePrice'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/iowa_train.csv')\n",
    "df_test = pd.read_csv('data/iowa_test.csv')\n",
    "\n",
    "# Drop houses where the target value for prediction is not defined (!)\n",
    "df_train.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "\n",
    "print(\"Train size={} Test size={}\".format(df_train.shape[0], df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns with missing values\n",
    "# Also removing Id and SalesPrice as these columns does not matter \n",
    "cols_with_missing = [col for col in df_train.columns \n",
    "                                 if df_train[col].isnull().any()]                                  \n",
    "\n",
    "# Loading predictors and target (X/y) \n",
    "X_train = df_train.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\n",
    "y_train = df_train.SalePrice\n",
    "X_test  = df_test.drop(['Id'] + cols_with_missing, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of columns used as predictors = 57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RoofStyle        object\n",
       "HeatingQC        object\n",
       "MSSubClass        int64\n",
       "Fireplaces        int64\n",
       "HouseStyle       object\n",
       "ExterQual        object\n",
       "MiscVal           int64\n",
       "LandSlope        object\n",
       "EnclosedPorch     int64\n",
       "BsmtUnfSF         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now just considering numeric columns and columns with low cardinality\n",
    "# \"cardinality\" means the number of unique values in a column.\n",
    "# We use it as our only way to select categorical columns here. This is convenient, though\n",
    "# a little arbitrary.\n",
    "low_cardinality_cols = [cname for cname in X_train.columns if \n",
    "                                X_train[cname].nunique() < 10 and\n",
    "                                X_train[cname].dtype == \"object\"]\n",
    "numeric_cols = [cname for cname in X_train.columns if \n",
    "                                X_train[cname].dtype in ['int64', 'float64']]\n",
    "cols = low_cardinality_cols + numeric_cols\n",
    "\n",
    "X_train = X_train[cols]\n",
    "X_test  = X_test[cols]\n",
    "print(\"Amount of columns used as predictors = {}\".format(len(cols)))\n",
    "X_train.dtypes.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to return MAE using cross_val_score (k-folding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(X, y):\n",
    "    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n",
    "    return -1 * cross_val_score(RandomForestRegressor(50), \n",
    "                                X, y, \n",
    "                                scoring = 'neg_mean_absolute_error').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying some different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of SalePrice when Dropping Categoricals = 18,465\n"
     ]
    }
   ],
   "source": [
    "# Model 1 - Not considering categorical columns\n",
    "X_train_new = X_train.select_dtypes(exclude=['object'])\n",
    "\n",
    "print('MAE of SalePrice when Dropping Categoricals = {:,.0f}'.format(get_mae(X_train_new, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of SalePrice when Dropping Categoricals = 18,036\n"
     ]
    }
   ],
   "source": [
    "# Model 2 - Using One-Hot Encoding (get_dummies)\n",
    "X_train_new = pd.get_dummies(X_train)\n",
    "\n",
    "print('MAE of SalePrice when Dropping Categoricals = {:,.0f}'.format(get_mae(X_train_new, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 - Considering data from test dataset also\n",
    "\n",
    "X_train_new = pd.get_dummies(X_train)\n",
    "X_test_new = pd.get_dummies(X_test)\n",
    "a,b = X_train_new.align(X_test_new, join='left', axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
